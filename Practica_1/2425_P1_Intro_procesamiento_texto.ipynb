{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "collapsed_sections": [
    "2NrEE6fcgEpP",
    "Sg-FkToOhuGQ",
    "Iz37vkmwnK7P",
    "dQikxXZxn6c3",
    "u22w3BFiOveA",
    "DMTAs5WJQE5h",
    "Fuc95vvxPdOv"
   ]
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PiewiqJVPI6D"
   },
   "source": [
    "# **Práctica 1. Introducción al Procesamiento de Texto**"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Introducción a Python"
   ],
   "metadata": {
    "id": "2NrEE6fcgEpP"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Aunque la sintaxis de Python permite muchas variaciones, existe una guía de estilo que es recomendable para todo programador de Python. Cuando acabe el curso y antes de empezar a usar Python como tu lenguaje de programación es más que aconsejable leerla atentamente:\n",
    "\n",
    "[Guía de estilo de Python: PEP 8](https://www.python.org/dev/peps/pep-0008/)\n",
    "\n",
    "Otras guías de estilo:\n",
    "\n",
    "* [Guía de estilo de Python de Google](https://google.github.io/styleguide/pyguide.html).\n",
    "\n",
    "A continuación, algunos conceptos básicos de Python para crear nuestros primeros programas."
   ],
   "metadata": {
    "id": "isOR1uZ1gTCX"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Comentarios\n",
    "\n",
    "Para comentar código se utiliza la almohadilla (#). También puedes comentar múltiples líneas de código seleccionándolas y pulsando **Ctrl+Ç**."
   ],
   "metadata": {
    "id": "8PKMT04dgqyQ"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Variables\n",
    "\n",
    "Python es un lenguaje **NO** tipado, lo que quiere decir que el intérprete deducirá el tipo de dato de una variable de forma dinámica. Por lo tanto, una variable puede cambiar de tipo en cualquier momento.\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "mi_variable = 1                 # Entero\n",
    "mi_variable = True              # Booleano\n",
    "mi_variable = 3e-3              # Flotante\n",
    "mi_variable = \"Hola mundo!\"     # Cadena\n",
    "```\n",
    "\n",
    "Utilizando ```type``` podemos saber el tipo de una variable en un momento concreto.\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "type(mi_variable)\n",
    "```\n",
    "\n"
   ],
   "metadata": {
    "id": "j1mhmcGXgvbP"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Operadores\n",
    "\n",
    "**Aritméticos:**\n",
    "\n",
    "* Suma: +\n",
    "* Resta: -\n",
    "* Multiplicación: *\n",
    "* División: /\n",
    "* Módulo: %\n",
    "* Exponenciación: **\n",
    "* División entera: //\n",
    "\n",
    "**De comparación:**\n",
    "\n",
    "* Igual: ==\n",
    "* Distinto: !=\n",
    "* Mayor: >\n",
    "* Menor: <\n",
    "* Mayor o igual: >=\n",
    "* Menor o igual: <=\n",
    "\n",
    "**Lógicos:**\n",
    "\n",
    "* and: Devuelve True si ambos valores son verdaderos.\n",
    "* or: Devuelve True si solo uno de los valores es verdadero.\n",
    "* not: Invierte el valor del booleano.\n",
    "\n",
    "NOTA: `&`, `|`, `^`, y `~` en Python son operadores binarios, no son equivalentes a `and`, `or`, `xor` (este no existe en Python) o `not`."
   ],
   "metadata": {
    "id": "bq9hwgaShazY"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Listas y Tuplas\n",
    "\n",
    "Otro tipo de datos muy importante que vamos a usar son las secuencias: tuplas y listas. Ambos son conjuntos ordenados de elementos: las tuplas se demarcan con paréntesis ( ) y las listas con corchetes [ ].\n",
    "\n",
    "Diferencias:\n",
    "* Una lista puede ser alterada, una tupla no. Las tuplas son \"inmutables\".\n",
    "* Una tupla puede ser utilizada como clave en un diccionario, una lista no.\n",
    "* Una tupla consume menos memoria que una lista.\n",
    "\n",
    "Algunos ejemplos:"
   ],
   "metadata": {
    "id": "Sg-FkToOhuGQ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "mi_lista = [1, 2, \"5\", 2]\n",
    "mi_tupla = (1, 2, \"5\", 2)"
   ],
   "metadata": {
    "id": "O1mUacrOjBGH"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Podemos comprobar si un elemento está o no dentro de una secuencia\n",
    "print(2 in mi_lista)\n",
    "print(2 not in mi_tupla)"
   ],
   "metadata": {
    "id": "lBnjWVrLkb7W"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Usamos len() para extraer la cantidad de elementos de la secuencia.\n",
    "print(len(mi_lista))\n",
    "print(len(mi_tupla))"
   ],
   "metadata": {
    "id": "kwLGf7-jkaJf"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "mi_lista.append(\"A\") # Añade el caracter A al final de la lista\n",
    "mi_lista.extend([\"B\", \"C\"]) # Añade los caracteres B y C al final\n",
    "mi_lista.insert(0, \"D\") # Añade el caracter D en la posición 0 (al principio)\n",
    "mi_lista.remove(2) # Elimina la primera ocurrencia del elemento 2\n",
    "dato = mi_lista.pop(0) # Extrae el primer elemento y lo devuelve\n",
    "dato = mi_lista.pop() # Por defecto, extrae el último elemento. Igual que mi_lista.pop(-1)\n",
    "\n",
    "print(mi_lista)\n",
    "print(dato)"
   ],
   "metadata": {
    "id": "fYYb1Vp5kWt_"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Concatenar listas y tuplas\n",
    "lista_1 = [1, 2, 3]\n",
    "lista_2 = [4, 5, 6]\n",
    "lista3 = lista_1 + lista_2\n",
    "print(lista3)\n",
    "\n",
    "tupla_1 = (1, 2, 3)\n",
    "tupla_2 = (4, 5, 6)\n",
    "tupla_3 = tupla_1 + tupla_2\n",
    "print(tupla_3)"
   ],
   "metadata": {
    "id": "RuMdErb8kVFA"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# También podemos hacer listas de listas\n",
    "bolas = ['roja', 'negra', 'blanca', 'azul']\n",
    "estudiantes = ['Rosa', 'Antonio', 'Ismael', 'Anabel']\n",
    "edades = [15, 18, 25, 35]\n",
    "\n",
    "lista_de_listas = [bolas, estudiantes, edades]\n",
    "\n",
    "print(lista_de_listas)"
   ],
   "metadata": {
    "id": "pU4RjB0Skc4_"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Para buscar y ordenar también tenemos varios métodos\n",
    "estudiantes = ['Rosa', 'Antonio', 'Ismael', 'Anabel', 'Miguel', 'Cristina', 'Lucas', 'Miguel']\n",
    "\n",
    "estudiantes.reverse()   # Invierte el orden de los elementos\n",
    "print(\".reverse()\", estudiantes)\n",
    "\n",
    "estudiantes.sort()      # Ordena los elementos (alfabéticamente para str)\n",
    "print(\".sort()\", estudiantes)\n",
    "\n",
    "estudiantes.sort(reverse=True)  # Ordena los elementos en orden inverso\n",
    "print(\".sort(reverse=True)\", estudiantes)\n",
    "\n",
    "print(f\"Miguel aparece {estudiantes.count('Miguel')} veces.\")   # Cuenta el número de apariciones del elemento buscado\n",
    "print(f\"Miguel aparece en la posición {estudiantes.index('Miguel')}\")   # Extrae la posición del elemento buscado"
   ],
   "metadata": {
    "id": "lFtwVla6kjrf"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Rangos\n",
    "\n",
    "Los rangos son tipos especiales en Python que devuelven un objeto que produce una secuencia de enteros desde `start` (incluido) hasta `stop` (no incluido) saltando `step` (opcional). Si solo se especifica un valor, Python lo interpretará como el valor de `stop`, y `start` valdrá 0.\n",
    "\n",
    "Son especialmente útiles para iterar por ellos dentro de un bucle for."
   ],
   "metadata": {
    "id": "Iz37vkmwnK7P"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "print(list(range(6)))\n",
    "print(list(range(0, 6, 2)))\n",
    "print(list(range(5, -1, -1)))"
   ],
   "metadata": {
    "id": "NqfKXYJapSrg"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Diccionarios\n",
    "\n",
    "En Python, un diccionario es una colección no-ordenada de valores que son accedidos a traves de una clave. Es decir, en lugar de acceder a la información mediante el índice numérico (posición), como es el caso de las listas y tuplas, es posible acceder a los **valores** a través de sus **claves**, que pueden ser de diversos tipos.\n",
    "\n",
    "Las claves son **únicas** dentro de un diccionario, es decir que no puede haber un diccionario que tenga dos veces la misma clave, si se asigna un valor a una clave ya existente, se reemplaza el valor anterior.\n",
    "\n",
    "No hay una forma directa de acceder a una clave a través de su valor, y nada impide que un mismo valor se encuentre asignado a distintas claves.\n",
    "\n",
    "La informacion almacenada en los diccionarios no tiene un orden particular. Ni por clave, ni por valor, ni tampoco por el orden en que han sido agregados al diccionario.\n",
    "\n",
    "Cualquier variable de tipo **inmutable, puede ser clave** de un diccionario: cadenas, enteros, tuplas (con valores inmutables en sus miembros), etc. **No hay restricciones para los valores** que el diccionario puede contener, cualquier tipo puede ser el valor: listas, cadenas, tuplas, otros diccionarios, objetos...\n",
    "\n",
    "De la misma forma que con listas, es posible definir un diccionario directamente con los miembros que va a contener, o bien inicializar el diccionario vacío y luego agregar los valores de uno en uno o de muchos en muchos.\n",
    "\n",
    "Para definirlo junto con los miembros que va a contener, se encierra el listado de valores entre llaves, las parejas de clave y valor se separan con comas, y la clave y el valor se separan con \":\"."
   ],
   "metadata": {
    "id": "dQikxXZxn6c3"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "punto = {\"x\": 2, \"y\": 1, \"z\": 4}\n",
    "\n",
    "materias = {}\n",
    "materias[\"lunes\"] = [6103, 7540]\n",
    "materias[\"martes\"] = [6201]\n",
    "materias[\"miercoles\"] = [6103, 7540]\n",
    "materias[\"jueves\"] = []\n",
    "materias[\"viernes\"] = [6201]\n",
    "\n",
    "# Para acceder al valor asociado a una determinada clave, se hace de la misma\n",
    "# forma que con las listas, pero utilizando la clave elegida en lugar del índice.\n",
    "\n",
    "valor = materias[\"lunes\"]\n",
    "print(valor)\n",
    "\n",
    "# También se puede acceder a los valores de un diccionario con el método\n",
    "# \"get(key, value)\". Si la clave no existe, devuelve value.\n",
    "\n",
    "valor = materias.get(\"sábado\", [777])\n",
    "print(valor)"
   ],
   "metadata": {
    "id": "tJf5KytLph9_"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Estructuras de control\n",
    "\n",
    "En Python los bloques se delimitan usando el indentado, utilizando siempre cuatro espacios (esto de cuatro es una norma de estilo). Cuando ponemos los dos puntos al final de la primera línea del condicional, todo lo que vaya a continuación con un nivel de indentado superior se considera dentro del condicional. En cuanto escribimos la primera línea con un nivel de indentado inferior, hemos cerrado el condicional. Si no seguimos esto a rajatabla, Python nos dará errores. Es una forma de forzar a que el código sea legible.\n",
    "\n",
    "1. Condicionales\n",
    "```\n",
    "if <condición>:\n",
    "        <haz lo que sea>\n",
    "elif <condición>:\n",
    "        <haz otra cosa>\n",
    "else:\n",
    "        <haz otra cosa>\n",
    "```\n",
    "\n",
    "2. Bucle while\n",
    "```\n",
    "while <condición>:\n",
    "        <cosas que hacer>\n",
    "```\n",
    "\n",
    "3. Bucle for\n",
    "```\n",
    "for <elemento> in <objeto iterable>:\n",
    "        <haz lo que sea...>\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "PAzOvbo9lrl_"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Funciones\n",
    "\n",
    "En Python, la definición de funciones se realiza mediante la instrucción def más un nombre de función descriptivo, para el que se aplican las mismas reglas que para el nombre de las variables, seguido de los paréntesis de apertura y cierre. La definición de la cabecera de la función termina con dos puntos (:). El algoritmo que la compone, irá indentado con 4 espacios:"
   ],
   "metadata": {
    "id": "P0sx-8kyqAse"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def mi_funcion():\n",
    "    print('¡Hola, mundo!')\n",
    "\n",
    "mi_funcion()"
   ],
   "metadata": {
    "id": "rHgC0db6qE9m"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "A la hora de definir la función pueden especificarse tantos argumentos o parámetros de entrada como sean necesarios, que pueden o no tener valores por defecto."
   ],
   "metadata": {
    "id": "AOI4j5sOqGAW"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def saludar(x, y=\"Lennon\"):\n",
    "    saludo = f\"¡Hola, {x} {y}!\"\n",
    "    return saludo\n",
    "\n",
    "nombre, apellido = \"John\", \"Doe\"\n",
    "\n",
    "# Especificando apellido\n",
    "saludo = saludar(nombre, apellido)\n",
    "print(saludo)\n",
    "\n",
    "# Sin especificar apellido, toma el valor por defecto\n",
    "saludo = saludar(nombre)\n",
    "print(saludo)"
   ],
   "metadata": {
    "id": "GDqcJR43qSSm"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cKSi5pFNr-88"
   },
   "source": [
    "### Ficheros\n",
    "\n",
    "Python tiene varios modos de lectura y escritura que se especifican como parámetro de la función `open()`:\n",
    "\n",
    "\n",
    "\n",
    "* \"r\": Read - Valor por defecto. Abre un fichero existente, devuelve un error si no existe.\n",
    "* \"a\": Append - Abre un fichero existente para añadir contenido, crea el fichero si no existe.\n",
    "* \"w\": Write - Abre un fichero para escribir contenido, crea el fichero si no existe.\n",
    "* \"x\": Create - Crea un nuevo fichero, devuelve un error si existe previamente.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u22w3BFiOveA"
   },
   "source": [
    "#### Activar Google Drive\n",
    "\n",
    "En los ejemplos siguientes se muestra cómo activar Google Drive en tu entorno de ejecución con un código de autorización y cómo puedes escribir y leer archivos en ese entorno. Cuando se haya ejecutado, podrás ver el nuevo archivo &#40;<code>cancion_del_pirata.txt</code>&#41; en <a href=\"https://drive.google.com/\">https://drive.google.com/</a>."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ],
   "metadata": {
    "id": "a4eXe5bbsb_X"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vVk1DoWz1O7a"
   },
   "source": [
    "#### Escribir datos"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Modo write (\"w\"):"
   ],
   "metadata": {
    "id": "QLXk9ac0S4Im"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "w9mD7dGR1O7T"
   },
   "source": [
    "nombre_fichero = \"cancion_del_pirata.txt\"\n",
    "\n",
    "with open(f\"{nombre_fichero}\", \"w\") as f:\n",
    "    f.write(\"Con diez cañones por banda\\n\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "```\n",
    "# Alternativa:\n",
    "f = open(f\"{nombre_fichero}\", \"w\")\n",
    "f.write(\"Con diez cañones por banda\\n\")\n",
    "f.close()\n",
    "```"
   ],
   "metadata": {
    "id": "Z0WvDlfEsrL3"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Modo append (\"a\"):"
   ],
   "metadata": {
    "id": "IvecwZPmTE73"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ARzX1rgH1O7d"
   },
   "source": [
    "# Abre el fichero en modo append y escribe otra línea\n",
    "with open(f\"{nombre_fichero}\", \"a\") as f:\n",
    "    f.write(\"viento en popa a toda vela\\n\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "```\n",
    "# Alternativa:\n",
    "f = open(f\"{nombre_fichero}\", \"a\")\n",
    "f.write(\"viento en popa a toda vela\\n\")\n",
    "f.close()\n",
    "```\n",
    "\n"
   ],
   "metadata": {
    "id": "SFMvija9sw0R"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Leer datos"
   ],
   "metadata": {
    "id": "ozUWDSw97q1k"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Modo read (\"r\"):"
   ],
   "metadata": {
    "id": "J5RXcSUUTNNX"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "La función `read()` devuelve el contenido completo de un fichero."
   ],
   "metadata": {
    "id": "YXEziV2YB97y"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "with open(f\"{nombre_fichero}\", \"r\") as f:\n",
    "    contenido = f.read()\n",
    "\n",
    "print(contenido)"
   ],
   "metadata": {
    "id": "G-qR9z7B7srM"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "```\n",
    "# Alternativa:\n",
    "f = open(f\"{nombre_fichero}\", \"r\")\n",
    "contenido = f.read()\n",
    "f.close()\n",
    "\n",
    "print(contenido)\n",
    "```\n",
    "\n"
   ],
   "metadata": {
    "id": "T24dVYfQs0kH"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "La función `readlines()` devuelve una lista de líneas dentro de un fichero."
   ],
   "metadata": {
    "id": "oX5Yhj-cCFyw"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "with open(f\"{nombre_fichero}\", \"r\") as f:\n",
    "    contenido = f.readlines()\n",
    "\n",
    "print(contenido)"
   ],
   "metadata": {
    "id": "fBNA-vXxB6Sq"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "```\n",
    "# Alternativa:\n",
    "f = open(f\"{nombre_fichero}\", \"r\")\n",
    "contenido = f.readlines()\n",
    "f.close()\n",
    "\n",
    "print(contenido)\n",
    "```\n",
    "\n"
   ],
   "metadata": {
    "id": "i86IY2avtDAH"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Bibliotecas\n",
    "\n",
    "Una biblioteca es un conjunto de módulos que contienen código que puede ser reutilizado en diferentes programas. Python cuenta con una gran variedad de bibliotecas de forma nativa, pero es posible instalar muchas más mediante el comando de bash `pip install <biblioteca>` (en Colab se utiliza `!` para utilizar comandos de bash).\n",
    "\n",
    "Hay varias formas de importar una biblioteca:\n",
    "\n",
    "* `import <biblioteca>`\n",
    "* `import <biblioteca> as <pseudónimo>`\n",
    "* `from <biblioteca> import <módulo>`\n",
    "\n",
    "Algunas de las bibliotecas \"built-in\" más utilizadas son:\n",
    "\n",
    "* `os` - Funcionalidades dependientes del sistema operativo.\n",
    "* `math` - Funciones matemáticas.\n",
    "* `random` - Generación de números pseudo-aleatorios.\n",
    "* `datetime` - Funciones relacionadas con fechas.\n",
    "\n",
    "Algunas de las bibliotecas instalables más utilizadas son (al ser tan populares, Colab las tiene preinstaladas, pero para utilizarlas de forma local en tu ordenador sí tendrías que instalarlas:\n",
    "\n",
    "* `numpy` - Para utilizar arrays numéricos. Importado típicamente bajo el pseudónimo `np` (`import numpy as np`)\n",
    "* `pandas` - Para análisis de conjuntos de datos en ficheros csv, tsv, xlsl, etc. Importado típicamente bajo el pseudónimo `pd` (`import pandas as pd`)\n",
    "* `sklearn` - Para machine learning."
   ],
   "metadata": {
    "id": "UJxCwAzTtTaH"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from datetime import datetime\n",
    "print(f\"Fecha actual: {datetime.now()}\") # Extraer fechas\n",
    "\n",
    "import math\n",
    "print(math.sqrt(144))   # Raíz cuadrada\n",
    "\n",
    "import numpy as np\n",
    "array_aleatorio = np.random.rand(5) # Creación de un array de dimensión 5 con valores aleatorios\n",
    "print(array_aleatorio)\n",
    "print(array_aleatorio.argmax()) # Índice del valor más alto dentro del array"
   ],
   "metadata": {
    "id": "vextyh1qtXBH"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Preprocesamiento de texto"
   ],
   "metadata": {
    "id": "5F5W3v6cgJLH"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DMTAs5WJQE5h"
   },
   "source": [
    "### ¿Qué es **NLTK**?\n",
    "\n",
    "[NLTK](http://www.nltk.org/) es una librería que proporciona interfaces para utilizar fácilmente una gran cantidad de recursos léxicos, así como métodos para el procesamiento, análisis y clasificación de textos.\n",
    "\n",
    "La librería tiene asociado un libro, que además de instruir en su uso explica muchos conceptos de PLN: http://www.nltk.org/book/."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-HFMyVWJQosI"
   },
   "source": [
    "#### 1. Instalando NLTK en notebook\n",
    "\n",
    "Este notebook tiene algunas dependencias, la mayoría de las cuales se pueden instalar a través del gestor de paquetes de python `pip`.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ucZaAklAQ369"
   },
   "source": [
    "!pip install nltk"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Importación y Descarga de Recursos\n",
    "\n",
    "El uso de NLTK requiere de su importación. NLTK es más que una librería, dado que ofrece la descarga de recursos lingüísticos."
   ],
   "metadata": {
    "id": "Fuc95vvxPdOv"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import nltk\n",
    "nltk.download()"
   ],
   "metadata": {
    "id": "wedh_QS1P5-t",
    "collapsed": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gqTAibyxQS_e"
   },
   "source": [
    "### Preprocesamiento de un texto\n",
    "\n",
    "El preprocesamiento suele estar asociado a la *tokenización* y segmentación de texto. Para ello existen paquetes específicos, como es el caso del paquete *punkt*.\n",
    "\n",
    "Para su instalación debemos usar \"nltk.download('punkt')\"."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "RjzqTRAWQN5z"
   },
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g11TpIIjQbsr"
   },
   "source": [
    "#### 1. División del texto en oraciones\n",
    "\n",
    "El primer paso consiste en separar el texto en oraciones. Para ello, la librería NLTK proporciona la función:\n",
    "\n",
    "```\n",
    "sent_tokenize(text, language='english')\n",
    "```\n",
    "\n",
    "Esta función, divide en oraciones el texto pasado como argumento utilizando el  idioma que queremos analizar. Esta función utiliza un modelo de lenguaje incluyendo caracteres que marcan el inicio y el fin de una oración, y están disponibles para 17 lenguas europeas (español, inglés, holandés, francés...).\n",
    "Por defecto, si no se especifica ningún idioma, se utiliza el modelo en inglés.\n",
    "\n",
    "Veamos un ejemplo. En primer lugar, importamos la función *sent_tokenize* y después la llamamos pasándole como argumento el texto que queremos dividir. El tipo de dato que devuelve es una lista con las oraciones del texto."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Pti_1PXvRPKK"
   },
   "source": [
    "from nltk.tokenize import sent_tokenize"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "b7fyrXL5S0ED"
   },
   "source": [
    "text = \"Esto es una oración de prueba. ¿También divide las preguntas, Sr. Smith? Además, este tokenizador no separa por comas.\"\n",
    "sent_tokenize(text, language=\"spanish\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hnb-iPMZVdo9"
   },
   "source": [
    "#### 2. División de las oraciones en palabras (*tokenization*)\n",
    "\n",
    "Una vez separado el texto en oraciones vamos a ver cómo dividir una oración en palabras, concretamente en tokens. La forma básica de *tokenización* consiste en separar el texto en tokens por medio de espacios y signos de puntuación. Para ello, nosotros vamos a utilizar el tokenizador *TreebankWordTokenizer* (aunque hay muchos más).\n",
    "\n",
    "Lo primero que debemos hacer será importar el tokenizador y posteriomente instanciar la clase."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "3kjwtrrTVuFR"
   },
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "xT0zvqq6VvWf"
   },
   "source": [
    "tokenizer = TreebankWordTokenizer()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "1fi2BubCV9c1"
   },
   "source": [
    "text = \"Esto es una oración de prueba. ¿También divide las preguntas, Sr. Smith? Además, este tokenizador no separa por comas.\"\n",
    "tokenizer.tokenize(text)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HXbibxKoW-1e"
   },
   "source": [
    "NLTK proporciona otros tokenizadores como *RegexpTokenizer*, *WhitespaceTokenizer*, *SpaceTokenizer*, *WordPunctTokenizer*, etc., que deberéis probar para completar los ejercicios de esta práctica."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UZGqPHHMXStT"
   },
   "source": [
    "#### 3. Eliminación de palabras vacías (stop words)\n",
    "\n",
    "Las palabras vacías (*stop words*) son palabras que carecen de significado por sí solas. Suelen ser artículos, pronombres, preposiciones...\n",
    "\n",
    "En algunas tareas del Procesamiento del Lenguaje Natural resulta útil eliminar dichas palabras, por lo que a continuación vamos a ver cómo podríamos eliminar las palabras vacías que forman parte de un conjunto de tokens.\n",
    "\n",
    "NLTK cuenta con una lista de palabras vacías para diferentes idiomas. Veamos cómo se utiliza:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "wmsyNDFlWBvN"
   },
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "6fDte3XWb96H"
   },
   "source": [
    "spanish_stops = stopwords.words('spanish')\n",
    "print(spanish_stops)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "60nalenXcTJF"
   },
   "source": [
    "A continuación, dada una lista de palabras o tokens, vamos a filtrarlos para quitar aquellas palabras que son consideradas *stopwords*:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "5gsKQYHRcCpO"
   },
   "source": [
    "words = ['Esto', 'es', 'una', 'oración', 'de', 'prueba.', '¿También', 'divide', 'las', 'preguntas', ',', 'Sr.', 'Smith', '?', 'Además', ',', 'este', 'tokenizador', 'no', 'separa', 'por', 'comas', '.']\n",
    "\n",
    "\"\"\"\n",
    "filtered = []\n",
    "for word in words:\n",
    "#   if word not in spanish_stops:\n",
    "#     filtered.append(word)\n",
    "\"\"\"\n",
    "filtered = [word for word in words if word not in spanish_stops]\n",
    "\n",
    "print(filtered)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6_D_WJHvcyM2"
   },
   "source": [
    "#### 4. Reducción de las palabras a su raíz (stemming)\n",
    "\n",
    "*Stemming* es la técnica utilizada para eliminar los afijos de una palabra con el objetivo de obtener su raíz. Por ejemplo, la raíz de “biblioteca” es “bibliotec”.\n",
    "\n",
    "Este método se suele utilizar en los sistemas de recuperación de información para la indexación de palabras ya que, en lugar de almacenar todas las formas de una palabra, permite almacenar sólo las raíces, reduciendo el tamaño del índice y mejorando el resultado.\n",
    "\n",
    "Existen diferentes algoritmos de stemming: Porter Stemmer, Lancaster Stemmer, Snowball Stemmer...\n",
    "\n",
    "NLTK cuenta con una implementación de algunos de estos algoritmos que son muy fáciles de utilizar. Simplemente hay que instanciar la clase, por ejemplo, *PorterStemmer* y llamar al método *stem()* con la palabra para la cual deseamos obtener su raíz.\n",
    "\n",
    "A continuación, vamos a ver un ejemplo sobre cómo obtener las raíces de una lista de tokens utilizando el algoritmo *Snowball*:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "XyVl700XdZcH"
   },
   "source": [
    "from nltk.stem.snowball import SnowballStemmer"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "4tKO6C3Rd7d_"
   },
   "source": [
    "stemmer = SnowballStemmer(\"spanish\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "dsvwPucfd8ko"
   },
   "source": [
    "print(stemmer.stem(\"corriendo\"))\n",
    "print(stemmer.stem(\"biblioteca\"))\n",
    "print(stemmer.stem(\"aburridos\"))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 5. BPE: El algoritmo de tokenización de ChatGPT (y otros)\n",
    "\n",
    "Byte-Pair Encoding (BPE) se desarrolló inicialmente como un algoritmo para comprimir textos, y OpenAI lo utilizó para la tokenización durante el preentrenamiento del modelo GPT aunque hoy en día lo utilizan muchos otros modelos Transformer como la familia GPT, RoBERTa, Llama-3 o Gemma.\n",
    "\n",
    "BPE sustituye el par de elementos de mayor frecuencia por un nuevo elemento que no estaba contenido en el conjunto de datos inicial de forma iterativa hasta que se alcanza el tamaño de vocabulario deseado. Por ejemplo:\n",
    "\n",
    "Partiendo de un corpus con las siguiente 5 palabras:\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "\"hug\", \"pug\", \"pun\", \"bun\", \"hugs\"\n",
    "```\n",
    "\n",
    "El vocabulario base será `[\"b\", \"g\", \"h\", \"n\", \"p\", \"s\", \"u\"]`. En cada paso del entrenamiento de este tokenizador, el algoritmo buscará el par de tokens consecutivos más frecuente y los une. Así, la primera regla aprendida de este tokenizador sería: `(\"u\", \"g\") -> \"ug\"` pues este par aparece 3 veces en el corpus, dando como resultado el siguiente vocabulario actualizado: `[\"b\", \"g\", \"h\", \"n\", \"p\", \"s\", \"u\", \"ug\"]`. Este proceso se repite tantas veces como sea necesario hasta alcanzar el tamaño de vocabulario deseado (actualmente 8). El vocabulario de tokenizadores modernos suele rondar los 100k tokens.\n",
    "\n",
    "Pueden utilizarse tokenizadores preentrenados por OpenAI a través de la biblioteca Tiktoken (https://github.com/openai/tiktoken).\n",
    "\n",
    "```\n",
    "!pip install tiktoken\n",
    "\n",
    "import tiktoken\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-4o-mini\")\n",
    "\n",
    "encoded_text = encoding.encode(\"tiktoken is great!\")\n",
    "print(encoded_text)\n",
    "\n",
    "print(encoding.decode(encoded_text))\n",
    "print([encoding.decode_single_token_bytes(token) for token in encoded_text])\n",
    "\n",
    "```\n",
    "\n",
    "* Más información: https://huggingface.co/learn/nlp-course/en/chapter6/5\n",
    "\n",
    "* Herramienta de prueba: https://tiktokenizer.vercel.app/?model=gpt-3.5-turbo"
   ],
   "metadata": {
    "id": "hgoOa8sOAb6E"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install tiktoken\n",
    "\n",
    "import tiktoken\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-4o-mini\")\n",
    "\n",
    "encoded_text = encoding.encode(\"Fragmentación de oraciones mediante el tokenizador de GPT4o.\")\n",
    "\n",
    "print(\"Texto codificado:\", encoded_text)\n",
    "print(\"Texto decodificado:\", encoding.decode(encoded_text))\n",
    "print(\"Visualización de tokens independientes:\", [encoding.decode_single_token_bytes(token) for token in encoded_text])"
   ],
   "metadata": {
    "id": "eTr79Th_ImAV"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5fClmznfegbY"
   },
   "source": [
    "## Ejercicios"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "El resultado de esta primera práctica deberá entregarse en PLATEA y tiene como límite de entrega las **23:59 horas del día 17 de febrero de 2025**. Se entregará este mismo notebook de extensión *.ipynb* y se renombrará de la siguiente forma: pr1_usuario1_usuario2.ipynb. Sustituye \"usuario1\" y \"usuario2\" por el alias de vuestro correo.\n",
    "\n",
    "\n",
    "Para el desarrollo de estos ejercicios, debeis hacer uso de la colección de documentos de [SciELO](https://scielo.org/es/) disponible en PLATEA en la carpeta \"Material Complementario\" llamado \"colección_SciELO_PLN\".\n",
    "\n",
    "Esta colección está compuesta por 25 ficheros en formato XML. Debéis realizar el tratamiento de cada fichero y tener en cuenta el texto incluido en la etiqueta  **<dc:description xml:lang=\"es\">**"
   ],
   "metadata": {
    "id": "4jmVlWt9xTju"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VK-cub2Nhqmv"
   },
   "source": [
    "### Ejercicio 1\n",
    "\n",
    "Crear una función que divida en oraciones los textos, haciendo uso de la función\n",
    "*sent_tokenize*. La función mostrará el número medio de oraciones por cada fichero analizado, el nombre del fichero que contiene menos oraciones y el nombre del fichero que contiene más oraciones."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize"
   ],
   "metadata": {
    "id": "DFGPsd9MJOAa"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NehqwQRAhnWI"
   },
   "source": [
    "### Ejercicio 2\n",
    "\n",
    "Crear un programa que divida en oraciones los textos presentes en él. Posteriormente, realize una tokenización de las palabras haciendo uso de la clase *WordPunctTokenizer*. Finalmente, la función debe mostrar el número medio de palabras por fichero, el fichero que contiene menos palabras y el fichero que contiene más palabras."
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "JxgglwYcJObi"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lUey1NQvjBvt"
   },
   "source": [
    "### Ejercicio 3\n",
    "\n",
    "Dividir en tokens la oración que se muestra a continuación empleando los siguientes tokenizadores: “TreebankWordTokenizer”, \"WhitespaceTokenizer”, “SpaceTokenizer” y \"WordPunctTokenizer” de NLTK y `gpt-4o-mini` de Tiktoken.\n",
    "\n",
    "¿Qué diferencias se observan en la salida producida por cada uno de\n",
    "ellos?\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "sentence = \"Sorry, I can't go to the meeting.\\n\""
   ],
   "metadata": {
    "id": "Hm32XvHCJO4x"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C8JoaOaIjmGD"
   },
   "source": [
    "### Ejercicio 4\n",
    "\n",
    "Crear un tokenizador basado en expresiones regulares usando la clase *RegexpTokenizer* de NLTK que extraiga sólo las palabras presentes en el texto, es decir, que no devuelva como salida los signos de puntuación ni los tabuladores/saltos de línea, etc.\n",
    "\n",
    "Además, el tokenizador no deberá separar las contracciones del texto.\n",
    "\n",
    "¿Cuáles son los tokens extraídos si le pasamos la siguiente oración?\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "sentence = \"Sorry, I can't, go to the meeting.\\n\""
   ],
   "metadata": {
    "id": "knaTQmJjJPVj"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Ejercicio 5\n",
    "\n",
    "Usando el *corpus* SFU, compuesto por 400 documentos de opiniones de 8 dominios diferentes (libros, coches, ordenadores, utensilios de cocina, hoteles, pelícuas y teléfons), se debene realizar las siguientes operaciones:\n",
    "\n",
    "**Nota**: El *corpus* se encuentra en la sección \"Material Complementario\" de PLATEA.\n",
    "\n",
    "*   Mostrar el tamaño del vocabulario (*tokens* únicos) de cada dominio (2 tokenizaodres, uno de ellos basado en BPE).\n",
    "*   Mostrar el número total de palabras vacías por cada dominio (2 tokenizaodres, uno de ellos basado en BPE).\n",
    "*   Mostrar el porcentaje de palabras vacías en relación al número *tokens* únicos y de palabras *unicas* (sin signos de puntuación; 2 tokenizaodres, uno de ellos basado en BPE).\n",
    "*   Mostrar los 5 *stem* más comunes en cada dominio, evidentemente sin tener en cuenta las palabras vacías (2 tokenizaodres, uno de ellos basado en BPE).\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "Ylmj6CIWdkqB"
   }
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "ryWHi7ziJP0a"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
